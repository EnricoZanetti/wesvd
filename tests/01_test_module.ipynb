{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Word Embeddings from Co-occurrence Matrices**\n",
    "\n",
    "This notebook demonstrates how to generate word embeddings using co-occurrence matrices and Truncated SVD. We'll execute the main pipeline from `main.py` and explain each step.\n",
    "\n",
    "### **Step 1: Setup**\n",
    "First, we import the necessary modules and make the functions in `main.py` accessible. We also ensure that our working directory is set up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Ensure the main.py directory is in the path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "# Import the main script\n",
    "from src.main import compute_co_occurrence_matrix, distinct_words, plot_embeddings, reduce_to_k_dim\n",
    "from src.utils import read_corpus\n",
    "\n",
    "print('Libraries imported!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2: Loading the Corpus**\n",
    "\n",
    "We load the Reuters corpus using the `read_corpus` function from the `utils` module. This corpus is a collection of documents where each document is represented as a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenized Reuters corpus as a list of documents\n",
    "corpus = read_corpus()\n",
    "\n",
    "# Get the sorted list of distinct words and the total count\n",
    "corpus_words, num_distinct_words = distinct_words(corpus)\n",
    "\n",
    "# Print summary information about the corpus\n",
    "print(f'Number of documents in the corpus: {len(corpus)}')\n",
    "print(f'Number of distinct words: {num_distinct_words}')\n",
    "print(f'Sample document: {corpus[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3: Computing the Co-occurrence Matrix**\n",
    "\n",
    "A co-occurrence matrix captures how often words appear near each other within a specified context window.\n",
    "\n",
    "Each entry `(i, j)` in the matrix represents the frequency of word `j` appearing in the context of word `i`.\n",
    "\n",
    "- **Window size**: The number of words on either side of a target word to consider as context.\n",
    "- **Function used**: `compute_co_occurrence_matrix`.\n",
    "\n",
    "Letâ€™s compute the co-occurrence matrix for the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the co-occurrence matrix\n",
    "window_size = 4\n",
    "co_occurrence_matrix, word2ind = compute_co_occurrence_matrix(corpus, window_size)\n",
    "\n",
    "# Display the size of the matrix and a snippet of its contents\n",
    "print(f'Co-occurrence matrix shape: {co_occurrence_matrix.shape}')\n",
    "print(f'Sample words and their indices: {list(word2ind.items())[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4: Reducing Dimensionality with Truncated SVD**\n",
    "\n",
    "Co-occurrence matrices are usually high-dimensional and sparse, making them challenging to work with. To address this, we use **Truncated SVD** to reduce the dimensionality of the matrix to a smaller number of dimensions (`k`).\n",
    "\n",
    "- **Function used**: `reduce_to_k_dim`.\n",
    "- **Purpose**: Generate low-dimensional word embeddings that capture semantic relationships between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the dimensionality of the co-occurrence matrix\n",
    "k = 2\n",
    "reduced_matrix = reduce_to_k_dim(co_occurrence_matrix, k)\n",
    "\n",
    "# Display the reduced matrix\n",
    "print(f'Reduced matrix shape: {reduced_matrix.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 5: Normalizing Word Embeddings**\n",
    "\n",
    "To ensure uniformity and simplify comparisons, we normalize the rows of the reduced matrix so that each row (representing a word embedding) has a unit length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the rows of the reduced matrix\n",
    "row_norms = np.linalg.norm(reduced_matrix, axis=1, keepdims=True)\n",
    "normalized_embeddings = reduced_matrix / row_norms\n",
    "\n",
    "# Display the normalized embeddings for the first few words\n",
    "print('Sample normalized embeddings:')\n",
    "for word, idx in list(word2ind.items())[:5]:\n",
    "    print(f'{word}: {normalized_embeddings[idx]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 6: Visualizing Word Embeddings**\n",
    "\n",
    "We visualize the word embeddings for a selected set of words in a 2D space. This visualization helps us understand how semantically related words are positioned close to each other.\n",
    "\n",
    "- **Function used**: A helper function within `main.py` (`plot_embeddings`).\n",
    "- **Words to visualize**: `['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'venezuela']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the words to visualize\n",
    "words_to_visualize = [\n",
    "    'barrels',\n",
    "    'bpd',\n",
    "    'ecuador',\n",
    "    'energy',\n",
    "    'industry',\n",
    "    'kuwait',\n",
    "    'oil',\n",
    "    'output',\n",
    "    'petroleum',\n",
    "    'venezuela',\n",
    "]\n",
    "\n",
    "# Plot the embeddings\n",
    "plot_embeddings(normalized_embeddings, word2ind, words_to_visualize, '2D Word Embeddings')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conclusion**\n",
    "\n",
    "In this notebook, we demonstrated how to generate and visualize word embeddings using:\n",
    "\n",
    "1. A **co-occurrence matrix** to capture word relationships in a corpus.\n",
    "2. **Truncated SVD** to reduce the dimensionality of the co-occurrence matrix.\n",
    "3. **Normalization** to standardize the word embeddings.\n",
    "4. **Visualization** to understand semantic relationships in the embeddings.\n",
    "\n",
    "These embeddings can be further used for tasks like similarity measurement, clustering, or as input to machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XCS224N",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
